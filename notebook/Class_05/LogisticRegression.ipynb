{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d71077c-2c2e-4f47-8d9f-add9cd8a3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax_metrics as jm\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from functools import partial\n",
    "from jax import random\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Switch off the cache \n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'\n",
    "\n",
    "class Logistic_Regression():\n",
    "    \"\"\"\n",
    "    Basic Model + Quasi Newton Methods\n",
    "    \"\"\"\n",
    "    def __init__(self, regularization='l2', method_opt='classic_model'):\n",
    "        self.regularization = regularization\n",
    "        self.method_opt = method_opt\n",
    "        self.error_gradient = 0.001\n",
    "        self.key = random.PRNGKey(0)\n",
    "        # You need to add some variables\n",
    "        self.W = None\n",
    "\n",
    "    @staticmethod\n",
    "    @jit\n",
    "    def logistic_exp(W:jnp, X:jnp)->jnp:\n",
    "        \"\"\"\n",
    "        Generate all the w^T@x values \n",
    "        args:\n",
    "            W is a k-1 x d + 1\n",
    "            X is a d x N\n",
    "        \"\"\"\n",
    "        return jnp.exp(W@X)\n",
    "\n",
    "    @staticmethod\n",
    "    @jit\n",
    "    def logistic_sum(exTerms: jnp)->jnp:        \n",
    "        \"\"\"\n",
    "        Generate all the w^T@x values \n",
    "        args:\n",
    "            W is a k-1 x d \n",
    "            X is a d x N\n",
    "        \"\"\"\n",
    "        temp = jnp.sum(exTerms, axis=0)\n",
    "        n = temp.shape[0]\n",
    "        return jnp.reshape(1.0+temp, newshape=(1, n))\n",
    "\n",
    "    @staticmethod\n",
    "    @jit\n",
    "    def logit_matrix(Terms: jnp, sum_terms: jnp)->jnp:\n",
    "        \"\"\"\n",
    "        Generate matrix\n",
    "        \"\"\"\n",
    "        divisor = 1/sum_terms\n",
    "        n, _ = Terms.shape\n",
    "        replicate = jnp.repeat(divisor, repeats=n, axis=0 )\n",
    "        logits = Terms*replicate\n",
    "        return jnp.vstack([logits, divisor])\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def model(self, W:jnp, X:jnp, Y_hot:jnp)->jnp:\n",
    "        \"\"\"\n",
    "        Logistic Model\n",
    "        \"\"\"\n",
    "        W = jnp.reshape(W, self.sh)\n",
    "        terms = self.logistic_exp(W, X)\n",
    "        sum_terms = self.logistic_sum(terms)\n",
    "        matrix = self.logit_matrix(terms, sum_terms)\n",
    "        return jnp.sum(jnp.sum(jnp.log(matrix)*Y_hot, axis=0), axis=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def one_hot(Y: jnp):\n",
    "        \"\"\"\n",
    "        One_hot matrix\n",
    "        \"\"\"\n",
    "        numclasses = len(jnp.unique(Y))\n",
    "        return jnp.transpose(jax.nn.one_hot(Y, num_classes=numclasses))\n",
    "    \n",
    "    def generate_w(self, k_classes:int, dim:int)->jnp:\n",
    "        \"\"\"\n",
    "        Use the random generator at Jax to generate a random generator to instanciate\n",
    "        the augmented values\n",
    "        \"\"\"\n",
    "        key = random.PRNGKey(0)\n",
    "        keys = random.split(key, 1)\n",
    "        return jnp.array(random.normal(keys[0], (k_classes, dim)))\n",
    "\n",
    "    @staticmethod\n",
    "    def augment_x(X: jnp)->jnp:\n",
    "        \"\"\"\n",
    "        Augmenting samples of a dim x N matrix\n",
    "        \"\"\"\n",
    "        N = X.shape[1]\n",
    "        return jnp.vstack([X, jnp.ones((1, N))])\n",
    "     \n",
    "   \n",
    "    def fit(self, X: jnp, Y:jnp)->None:\n",
    "        \"\"\"\n",
    "        The fit process\n",
    "        \"\"\"\n",
    "        nclasses = len(jnp.unique(Y))\n",
    "        X = self.augment_x(X)\n",
    "        dim = X.shape[0]\n",
    "        W = self.generate_w(nclasses-1, dim)\n",
    "        Y_hot = self.one_hot(Y)\n",
    "        self.W = getattr(self, self.method_opt, lambda W, X, Y_hot: self.error() )(W, X, Y_hot)\n",
    "    \n",
    "    @staticmethod\n",
    "    def error()->None:\n",
    "        \"\"\"\n",
    "        Only Print Error\n",
    "        \"\"\"\n",
    "        raise Exception(\"Opt Method does not exist\")\n",
    "    \n",
    "    def classic_model(self, W:jnp, X:jnp, Y_hot:jnp, alpha:float=1e-2,  tol:float=1e-3)->jnp:\n",
    "        \"\"\"\n",
    "        The naive version of the logistic regression\n",
    "        \"\"\"\n",
    "        n, m = W.shape \n",
    "        self.sh = (n, m)\n",
    "        alpha = 0.5\n",
    "        Grad = jax.grad(self.model, argnums=0)(jnp.ravel(W), X, Y_hot)\n",
    "        loss = self.model(jnp.ravel(W), X, Y_hot)\n",
    "        cnt = 0\n",
    "        while True:\n",
    "            Hessian = jax.hessian(self.model, argnums=0)(jnp.ravel(W), X, Y_hot)\n",
    "            W = W - alpha*jnp.reshape((jnp.linalg.inv(Hessian)@Grad), self.sh)\n",
    "            Grad =  jax.grad(self.model, argnums=0)(jnp.ravel(W), X, Y_hot)\n",
    "            old_loss = loss\n",
    "            loss = self.model(jnp.ravel(W), X, Y_hot)\n",
    "            if cnt%30 == 0:\n",
    "                print(f'{self.model(jnp.ravel(W), X, Y_hot)}')\n",
    "            if  jnp.abs(old_loss - loss) < tol:\n",
    "                break\n",
    "            cnt +=1\n",
    "        return W\n",
    "    \n",
    "    def estimate_prob(self, X:jnp)->jnp:\n",
    "        \"\"\"\n",
    "        Estimate Probability\n",
    "        \"\"\"\n",
    "        X = self.augment_x(X)\n",
    "        terms = self.logistic_exp(self.W, X)\n",
    "        sum_terms = self.logistic_sum(terms)\n",
    "        matrix = self.logit_matrix(terms, sum_terms)\n",
    "        return matrix\n",
    "    \n",
    "    def estimate(self, X:jnp)->jnp:\n",
    "        \"\"\"\n",
    "        Estimation\n",
    "        \"\"\"\n",
    "        X = self.augment_x(X)\n",
    "        terms = self.logistic_exp(self.W, X)\n",
    "        sum_terms = self.logistic_sum(terms)\n",
    "        matrix = self.logit_matrix(terms, sum_terms)\n",
    "        return jnp.argmax(matrix, axis=0)\n",
    "    \n",
    "    def precision(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Precision\n",
    "        args:\n",
    "            y: Real Labels\n",
    "            y_hat: estimated labels\n",
    "        return TP/(TP+FP)\n",
    "        \"\"\"\n",
    "        TP = sum(y_hat == y)\n",
    "        FP = sum(y_hat != y)\n",
    "        return (TP/(TP+FP)).tolist()\n",
    "    \n",
    "class Tools():\n",
    "    \"\"\"\n",
    "    Tools\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Basic Init\n",
    "        \"\"\"\n",
    "        self.key = random.PRNGKey(0)\n",
    "    \n",
    "    def GenerateData(self, n_samples: int, n_classes: int, dim: int)-> (jnp, jnp):\n",
    "        \"\"\"\n",
    "        Data Generation\n",
    "        \"\"\"\n",
    "        Total_Data = [] \n",
    "        Total_Y = []\n",
    "        for idx in range(n_classes):\n",
    "            keys = random.split(self.key, 1)\n",
    "            X = random.normal(keys[0], (dim, n_samples)) + idx*5*jnp.ones((dim, 1))\n",
    "            Y = idx*jnp.ones(n_samples)\n",
    "            Total_Data.append(X)\n",
    "            Total_Y.append(Y)\n",
    "        return jnp.hstack(Total_Data), jnp.hstack(Total_Y)\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_classes(X: jnp, Y: jnp, n_classes: int)-> None:\n",
    "        \"\"\"\n",
    "        Plot the classes\n",
    "        \"\"\"\n",
    "        symbols = ['ro', 'bx', 'go', 'rx']\n",
    "        plt.figure()\n",
    "        for idx in range(n_classes):\n",
    "            mask = idx == Y\n",
    "            X_p = X[:, mask]\n",
    "            plt.plot(X_p[0,:], X_p[1,:], symbols[idx])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a6b01c-3d5a-4f96-bb3a-0ffea9ab9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = Tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98746205-096c-4934-9479-f99a9e7f6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = tools.GenerateData(n_samples=200, n_classes=3, dim=2)\n",
    "X_val, Y_val = tools.GenerateData(n_samples=50, n_classes=3, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f35aa37-9500-4e9d-bde7-7b94f934053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Logistic_Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37b0f67-e377-47eb-b260-e1236bb18422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-676.3422241210938\n",
      "-7.649104118347168\n",
      "-7.283784866333008\n",
      "-7.024961471557617\n",
      "-6.799328804016113\n",
      "-6.612099647521973\n",
      "-6.44949197769165\n",
      "-6.303953647613525\n",
      "-6.181393623352051\n",
      "-6.0711894035339355\n",
      "-5.972036838531494\n",
      "-5.879446029663086\n",
      "-5.792116165161133\n",
      "-5.710974216461182\n",
      "-5.639213562011719\n",
      "-5.573758125305176\n",
      "-5.511788845062256\n",
      "-5.4524431228637695\n",
      "-5.394635200500488\n",
      "-5.339287757873535\n",
      "-5.286327362060547\n",
      "-5.234570503234863\n",
      "-5.18397855758667\n",
      "-5.135310173034668\n",
      "-5.08912467956543\n",
      "-5.044164657592773\n",
      "-5.000809669494629\n",
      "-4.959855079650879\n",
      "-4.920346260070801\n",
      "-4.882826328277588\n",
      "-4.846807479858398\n",
      "-4.811931133270264\n",
      "-4.777647972106934\n",
      "-4.7440667152404785\n",
      "-4.712246894836426\n",
      "-4.680954933166504\n",
      "-4.650136947631836\n",
      "-4.619787216186523\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b15d7fa-ed3c-418e-8b4c-a8b58b54978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = model.estimate(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade3f3f7-663b-4823-a89d-a38a0675152e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a114b7d2-4ba9-4388-ad1a-20b62d0142ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999403953552"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.precision(Y_val, Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56ac7b06-f73f-44de-bbcb-7dee4ffa2307",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prob = model.estimate_prob(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3090a977-9c11-4921-a7c1-de770c984d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([6.9437830e-15, 7.8591262e-04, 9.9921405e-01], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prob[:, 402]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39aab5eb-1a76-4882-868e-e551e6547fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[9.99986291e-01, 9.99937177e-01, 9.99298930e-01, ...,\n",
       "        2.92186624e-14, 1.23954905e-12, 2.04031704e-14],\n",
       "       [1.37140905e-05, 6.27966074e-05, 7.01099576e-04, ...,\n",
       "        1.48824987e-03, 7.83724617e-03, 1.27292634e-03],\n",
       "       [3.46828927e-15, 5.33205750e-14, 4.09862855e-12, ...,\n",
       "        9.98511732e-01, 9.92162764e-01, 9.98727024e-01]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e289ee9-6299-471f-b4ee-bb0d53427102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
