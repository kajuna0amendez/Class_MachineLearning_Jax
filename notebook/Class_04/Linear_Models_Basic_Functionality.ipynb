{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640ac27-6760-412c-bffc-3ed5c75f7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax_metrics as jm\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from functools import partial\n",
    "from jax import random\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Switch off the cache \n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b0fd2e-d1f5-4714-9f18-e9ccb8d53d5e",
   "metadata": {},
   "source": [
    "# Basic Linear Model\n",
    "\n",
    "Class for Linear estimation with Canonical and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a47c36-3e3d-4dc2-964a-8cc5b9f3114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Model():\n",
    "    \"\"\"\n",
    "    Basic Linear Regression\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int) -> None:\n",
    "        self.dim = dim\n",
    "        self.key = random.PRNGKey(0)\n",
    "        self.cpus = jax.devices(\"cpu\")\n",
    "    \n",
    "    # The classic one\n",
    "    @staticmethod\n",
    "    def generate_canonical_estimator(X: jnp, y:jnp) -> jnp:\n",
    "        \"\"\"\n",
    "        Cannonical LSE error solution for the Linearly separable classes \n",
    "        args:\n",
    "            X: Data array at the GPU or CPU\n",
    "            y: Label array at the GPU \n",
    "        returns:\n",
    "            w: Weight array at the GPU or CPU\n",
    "        \"\"\"\n",
    "        return  jax.numpy.linalg.inv(jax.numpy.transpose(X)@X)@jax.numpy.transpose(X)@y\n",
    "    \n",
    "    @staticmethod\n",
    "    @jit\n",
    "    def linear_model(X: jnp, theta: jnp) -> jnp:\n",
    "        \"\"\"\n",
    "        Classic Linear Model. Jit has been used to accelerate the loops after the first one\n",
    "        for the Gradient Descent part\n",
    "        args:\n",
    "            X: Data array at the GPU or CPU\n",
    "            theta: Parameter w for weights and b for bias\n",
    "        returns:\n",
    "            f(x): the escalar estimation on vector x or the array of estimations\n",
    "        \"\"\"\n",
    "        w = theta[:-1]\n",
    "        b = theta[-1]\n",
    "        return jax.numpy.matmul(X, w) + b\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def LSE(self, theta: jnp, X: jnp, y: jnp)-> jnp:\n",
    "        \"\"\"\n",
    "        LSE in matrix form. We also use Jit por froze info at self to follow \n",
    "        the idea of functional programming on Jit for no side effects\n",
    "        args:\n",
    "            X: Data array at the GPU or CPU\n",
    "            theta: Parameter w for weights and b for bias\n",
    "            y: array of labels\n",
    "        returns:\n",
    "            the Loss function LSE under data X, labels y and theta initial estimation\n",
    "        \"\"\"\n",
    "        return (jax.numpy.transpose(y - self.linear_model(X, theta))@(y - self.linear_model(X, theta)))[0,0]\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def update(self, theta: jnp, X: jnp, y: jnp, lr: float)->jnp:\n",
    "        \"\"\"\n",
    "        Update makes use of the autograd at Jax to calculate the gradient descent.\n",
    "        args:\n",
    "            X: Data array at the GPU or CPU\n",
    "            theta: Parameter w for weights and b for bias\n",
    "            y: array of labels\n",
    "            lr: Learning rate for Gradient Descent\n",
    "        returns:\n",
    "            the step update w(n+1) = w(n)-δ(t)𝜵L(w(n))        \n",
    "        \"\"\"\n",
    "        return theta - lr * jax.grad(self.LSE)(theta, X, y)  \n",
    "\n",
    "    def generate_w_aug(self)->jnp:\n",
    "        \"\"\"\n",
    "        Use the random generator at Jax to generate a random generator to instanciate\n",
    "        the augmented values\n",
    "        \"\"\"\n",
    "        keys = random.split(self.key, 1)\n",
    "        return jax.numpy.vstack([random.normal(keys, (self.dim,1)), jax.numpy.array(1)])\n",
    "    \n",
    "    def generate_theta(self)->jnp:\n",
    "        \"\"\"\n",
    "        Use the random generator at Jax to generate a random generator to instanciate\n",
    "        the augmented values\n",
    "        \"\"\"\n",
    "        keys = random.split(self.key, 1)\n",
    "        return jax.numpy.vstack([random.normal(keys, (self.dim,1)), jax.numpy.array(0)])\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def estimate_grsl(self, X: jnp, theta: jnp)->jnp:\n",
    "        \"\"\"\n",
    "        Estimation for the Gradient Descent version\n",
    "        args:\n",
    "            X: Data array at the GPU or CPU\n",
    "            theta: Parameter w for weights and b for bias\n",
    "        return:\n",
    "            Estimation of data X under linear model\n",
    "        \"\"\"\n",
    "        w = theta[:-1]\n",
    "        b = theta[-1]\n",
    "        return X@w+b\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_cannonical(X: jnp, w: jnp)->jnp:\n",
    "        \"\"\"\n",
    "        Estimation for the Gradient Descent version\n",
    "        args:\n",
    "            X: Data array at the GPU or CPU\n",
    "            w: Parameter w under extended space\n",
    "        return:\n",
    "            Estimation of data X under cannonical solution\n",
    "        \"\"\"\n",
    "        return X@w\n",
    "    \n",
    "    def precision(self, y: jnp, y_hat: jnp)->float:\n",
    "        \"\"\"\n",
    "        Precision\n",
    "        args:\n",
    "            y: Real Labels\n",
    "            y_hat: estimated labels\n",
    "        return TP/(TP+FP)\n",
    "        \"\"\"\n",
    "        TP = sum(y_hat[y>0]>0)\n",
    "        FP = sum(y_hat[y>0]<0)\n",
    "        result = TP/(TP+FP)\n",
    "        precision_cpu = jax.jit(lambda x: x, device=self.cpus[0])(result)\n",
    "        return float(precision_cpu)\n",
    "    \n",
    "    def gradient_descent(self, theta: jnp,  X: jnp, y: jnp, n_steps: int, lr = 0.001)->jnp:\n",
    "        \"\"\"\n",
    "        Gradient Descent Loop for the LSE Linear Model\n",
    "        args:\n",
    "            X: Data array at the GPU or CPU\n",
    "            theta: Parameter w for weights and b for bias\n",
    "            y: array of labels\n",
    "            n_steps: number steps for the Gradient Loop\n",
    "            lr: Learning rate for Gradient Descent   \n",
    "        return:\n",
    "            Updated Theta\n",
    "        \"\"\"\n",
    "        for i in range(n_steps):\n",
    "            theta = self.update(theta, X, y, lr)\n",
    "        return theta\n",
    "    \n",
    "    def gradient_descent_armijo(self, theta: jnp,  X: jnp, y: jnp)->jnp:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Grad = jax.grad(self.LSE)(theta, X, y)\n",
    "        p = -Grad\n",
    "        m = jnp.dot(jnp.transpose(Grad), p)[0,0]\n",
    "        c = 0.000001\n",
    "        tau = 0.8\n",
    "        t = -c*m\n",
    "        alpha = 1\n",
    "        while True:\n",
    "            f_x = self.LSE(theta, X, y)\n",
    "            while f_x - self.LSE(theta + alpha*p, X, y)  < alpha*t:\n",
    "                alpha = tau*alpha\n",
    "            theta = self.update(theta, X, y, alpha)\n",
    "            pGrad = Grad\n",
    "            Grad = jax.grad(self.LSE)(theta, X, y)\n",
    "            if not jnp.linalg.norm(Grad) < jnp.linalg.norm(pGrad):\n",
    "                break\n",
    "            p = -Grad\n",
    "            m = jnp.dot(jnp.transpose(Grad), p)[0,0]\n",
    "            t = -c*m\n",
    "            alpha = 1\n",
    "            # print(jnp.linalg.norm(Grad))\n",
    "        return theta\n",
    "            \n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13cfd3-318e-49bc-a579-ff0b43963d41",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06495957-df3c-48d3-bd8e-07072328a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.normal(size=(100,2))\n",
    "X2 = np.random.normal(size=(100,2))+(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6c83e-450e-4a00-b6f6-34c4ff290c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(X1[:,0], X1[:,1], 'ro')\n",
    "plt.plot(X2[:,0], X2[:,1], 'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c3b46-20f9-4c31-98f2-9b9c958a5be7",
   "metadata": {},
   "source": [
    "# Expriments\n",
    "Data Augmentation is applied for the Cannonical version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdccc8f1-22a4-42db-b468-79ec7e45811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_e = np.hstack([X1, np.ones((100,1))])\n",
    "X2_e = np.hstack([X2, np.ones((100,1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bed5cf-b2ed-4a5f-9f2b-a382d8e7e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_e = np.vstack([X1_e, X2_e])\n",
    "X = np.vstack([X1, X2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18644880-ec04-426a-94d0-f36fed87bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.vstack([-np.ones((100, 1)), np.ones((100, 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8804812-ef6e-4141-90a3-6d7816f489c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear_Model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f7c02-d233-43c9-89ac-92e42b94f3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.generate_canonical_estimator(X_e, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b73eb-a203-459a-ac25-ecb0023f70a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.estimate_cannonical(X_e, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddbc45a-b830-4f19-a686-1a10decf67fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e314ee3a-e0c0-49fe-8a6d-134c20c2b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.precision(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d3a7a-776c-49d6-99a6-097388518ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = model.generate_theta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1fb48c-e939-4e96-897a-b489e1eac266",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = model.gradient_descent_armijo(theta, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dcece6-d00e-4578-9854-09b39ca49921",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.estimate_grsl(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20447402-4e78-4fc1-b8f5-a74f3bf9a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.precision(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a42b9-9455-4893-b9f4-9f51072c018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f90d1ee-326a-4228-9ed8-fd27cf2ea88d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
